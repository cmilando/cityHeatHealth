---
title: "Using Spatial Bayesian methods in `cityHeatHealth`"
output:
  rmarkdown::html_vignette:
    fig.retina: 2
    dev: svg
vignette: >
  %\VignetteIndexEntry{Using Spatial Bayesian methods in `cityHeatHealth`}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(cityHeatHealth)
```

Another way that this model can be solved is by using Bayesian inference, implemented in STAN. We are including this implementation here so that it makes sense why we are including it later on. 

The innovation here is combining the method of Armstrong 2014 with a spatial method, in this case BYM2 

We implemented the spatial bayesian method of BYM2 but instead of regular poisson as a conditional poisson (i.e., multinomial) which has performance gains that they articulate in Amrstrong.

This requires bringing in a shapefile, so you can define the network 

The standard application is using MCMC

You can also experiment with speeding things up (at the risk of less precise estimates) using the laplace or variational method. see Jack's notes as so what is going on here

```{r setup2, fig.height=5, fig.width=5}
library(data.table)
data("ma_exposure")
data("ma_deaths")

# create exposure matrix
exposure_columns <- list(
  "date" = "date",
  "exposure" = "tmax_C",
  "geo_unit" = "TOWN20",
  "geo_unit_grp" = "COUNTY20"
)

TOWNLIST <- c('CHELSEA', 'EVERETT', 'REVERE', 'MALDEN')

exposure <- subset(ma_exposure, TOWN20 %in%  TOWNLIST &
                     year(date) %in% 2012:2015)

exposure_mat <- make_exposure_matrix(exposure, exposure_columns)

# create outcome table
outcome_columns <- list(
  "date" = "date",
  "outcome" = "daily_deaths",
  "factor" = 'age_grp',
  "factor" = 'sex',
  "geo_unit" = "TOWN20",
  "geo_unit_grp" = "COUNTY20"
)
deaths   <- subset(ma_deaths, TOWN20 %in%  TOWNLIST & 
                     year(date) %in% 2012:2015)
deaths_tbl <- make_outcome_table(deaths,  outcome_columns)

# plot
data("ma_towns")

library(ggplot2)
local_shp <- subset(ma_towns, TOWN20 %in%  TOWNLIST)
ggplot(local_shp) + geom_sf(aes(fill = TOWN20))
```

Now get initial estimates for each `geo_unit`
```{r b2, fig.height=5, fig.width=5}
beta_l <- vector("list", 4) 
cr_l <- vector("list", 4) 
plot_l <- vector("list", 4)

cb_list <- vector("list", 4)
oo_list <- vector("list", 4)

for(bb in 1:4) {
  m1 <- condPois_1stage(
    subset(exposure_mat, TOWN20 == TOWNLIST[bb]),
    subset(deaths_tbl, TOWN20 == TOWNLIST[bb]))
  
  cb_list[[bb]] <- m1$`_`$out[[1]]$orig_basis
  oo_list[[bb]] <- m1$`_`$out[[1]]$outcomes
  
  beta_l[[bb]] <- m1$`_`$out[[1]]$orig_coef
  
  cr_l[[bb]] <- m1$`_`$out[[1]]$coef
  
  plot_l[[bb]] <- plot(m1)
  
}
mx <- do.call(cbind, beta_l) # COEFS NOT THE SAME
colnames(mx)  = TOWNLIST
mx

mcr <- do.call(cbind, cr_l)   # COEFS THE SAME
colnames(mcr)  = TOWNLIST
mcr

library(patchwork)
wrap_plots(plot_l)
```

the cr coefs are similar

the orig_coefs are not, which is why beta-wise implementation of SB_DLNM method doesn't work - because the don't have to be the same to produce similar curves.

So, instead of forcing Beta to be similar, we can use bym2

refs:
 
 * https://mc-stan.org/learn-stan/case-studies/icar_stan.html
 * https://link.springer.com/article/10.1186/1476-072X-4-31
 * https://github.com/stan-dev/example-models/blob/e5b7d9e2e9ecc375805c7e49e4a4d4c1882b5e3b/knitr/car-iar-poisson/bym2_predictor_plus_offset.stan#L4 

ok here's the ref of how LAPLACE works:

* https://mc-stan.org/cmdstanr/reference/model-method-laplace.html
* https://statmodeling.stat.columbia.edu/2023/02/08/implementing-laplace-approximation-in-stan-whats-happening-under-the-hood/

I think this makes for a good candidate because betas are normal and the model is not hierarchical

  # and here https://discourse.mc-stan.org/t/r-package-using-cmdstanr/32758
  # stan_file <- system.file("stan", "SB_CondPoisson.stan",
  #                          package = "cityHeatHealth")
  #
  # mod <- cmdstanr::cmdstan_model(stan_file,
  #    

```{r b3a}
m_sb1 <- condPois_sb(exposure_mat, deaths_tbl, local_shp, 
                     stan_type = 'mcmc',
                     verbose = 2,
                     stan_opts = list(refresh = 200),
                     use_spatial_model = 'none')

```

Compare, first you can see that with spatial_model = F, there is similarity in beta coefs
```{r compare2}
mx

m_sb1$`_`$beta_mat
```


### Compare with spatial model

using laplace in this case, but you could try mcmc

```{r b3}
m_sb2 <- condPois_sb(exposure_mat, deaths_tbl, local_shp, 
                     stan_type = 'laplace',
                     verbose = 2,
                     stan_opts = list(refresh = 200),
                     use_spatial_model = 'bym2')

```

Compare, now you can see these are different

```{r compare2b}
mx

m_sb2$`_`$beta_mat
```
### Compare with spatial model for leroux

using laplace in this case, but you could try mcmc

```{r b4}
m_sb3 <- condPois_sb(exposure_mat, 
                     deaths_tbl, local_shp, 
                     stan_type = 'laplace',
                     verbose = 2,
                     stan_opts = list(refresh = 200),
                     use_spatial_model = 'leroux')

```
As you can see, lots of smoothing to a central estimate !
```{r compare3}
mx

m_sb3$`_`$beta_mat
```

And you can also see that the leroux `q` value is quite high

```{r getQ}
subset(m_sb3$`_`$stan_summary, variable == 'q')
```

Other bayesian notes

>> need to update to be the number of unique networks, because right now there are too many degree of freedom

--> what if you did not just n = 1 neighbors. whats about n = 2 neighbors with
a weighting factor on neighbors that are 2 away? I think thins should work. but you'll have to rework the STAN code so there are fewer 

so you'll have to have windows
and update this   // *****
  matrix[K, J] beta = rep_matrix(mu, J) + beta_star;
  // *****
  and update the y[ ] to be compared to multiple versions
  this is the way that that comes back around
and the bigger smoothness, the fewer spatial networks 

--> right because again the problem of this is it doesn't actually reduce the number of the problem. so the level of smoothing yes is determined by the user by saying how many levels of neighbors to use but 

--> ah ha, the problem there is (just like EpiEstim) you won't actually get betas
for everyplace --> but wait, you actually can if you do the poor-man's recursion. so the point would be you can have a 2-degree neighbor beta expressed in each of the J places, and then you can do just like in otherwise the ys, which would then give you region-specific betas (much like daily R(t)s). 

--> and maybe the two stage can be a weighted average?

--> probably might make sense to do look at how other people do spatial-temporal Poisson rather than doing it yourself. ChatGPT had some ideas ("low rank")

--> and its going to do better than EpiEstim because its not going to scale with the dataset. if anything it will get easier with more data.

--> Essentially, the SB is a bridge between and 1 stage and a 2 stage
if you want more granularity than a 1 stage, but the data aren't strong enough to do a 2 stage even with blups, you can do an SB model, which is sortof like many overlapping 1 stage models of larger spatial groups fused together
