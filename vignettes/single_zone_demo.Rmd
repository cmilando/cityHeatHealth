---
title: "Heat-Health Associations in a Single Town using `cityHeatHealth`"
output:
  rmarkdown::html_vignette:
    fig.retina: 2
    dev: svg
vignette: >
  %\VignetteIndexEntry{Heat-Health Associations in a Single Town using `cityHeatHealth`}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Let's use built-in functions to examine heat-health relationships in a single geographic unit. 

For testing purposes, start with your largest geographic unit! 

```{r setup}
library(cityHeatHealth)
```

### Exposure data

First let's make the exposure matrix for this single zone. There is a simulated exposure dataset included in the package called `ma_exposure`, which lists **daily maximum** temperatures each day in each town.

```{r load_exposure} 
boston_exposure <- subset(ma_exposure, TOWN20 == 'BOSTON')
head(boston_exposure)
```

Notice how this dataset contains temperature for the whole year. This is a good starting place. 

Also notice that these data are messy, as is common in environmental data, there are both NA values and days where there are no measurements:
```{r exp_clean}
# Sept 17 2010 has NA data
boston_exposure[255:260,]

# July 13 2010 is missing
boston_exposure[190:195,]
```

In the next step we'll introduce a function which (a) cleans the data and (b) subsets to just warm season months (which in Boston is May through September). Its good to keep the full temperature range before this point so we can use extra data around the cutoffs to make sure the temperature matrix is full. 

First, define the column mapping. The main geographic unit of analysis (the `geo_unit`) is called `TOWN20`, however we may also want to aggregate in some later steps to the group level, so the grouping variable (`geo_unit_grp`) is called `COUNTY20`. Providing a named list in this way avoids having to rename columns throughout the process. 

```{r exp_cols}
exposure_columns <- list(
  "date" = "date",
  "exposure" = "tmax_C",
  "geo_unit" = "TOWN20",
  "geo_unit_grp" = "COUNTY20"
)
```

Next pass in your full temperature time-series in this step.
```{r convertExp}
boston_exposure_mat <- make_exposure_matrix(boston_exposure, exposure_columns)
head(boston_exposure_mat)
```

You can check that the two problems we saw before -- missing data and NA data -- are gone now
```{r exp_check}
# Sept 17 2010 now has NA data
boston_exposure_mat[138:142,]

# July 13 2010 is now not missing
boston_exposure_mat[72:77,]
```

Its probably a good idea to see if there is any systematic bias in the missingness (i.e., are the data "missing at random") but for the purposes of this tutorial we'll assume that they are (which is true since these are simulated data!).

It may also be a good idea to check that the exposure values generally look like you expect them to look (e.g., with some summary statistics), just to make sure you don't need to do anys additional pre-processing (e.g., if there are location-specific wildly high or low temperatures, this script as is will not catch those).

```{r exp_summary}
summary(boston_exposure_mat$tmax_C)
```

For a time-series of daily maximum temperatures in warm-season months in Boston MA, looks good!

### Outcome data

Next let's investigate the deaths dataset - you can see that this has town level daily deaths with by category for age group (0-17, 18-64, and 65+) as well as a binary-coded sex variable.

```{r load_deaths} 
boston_deaths   <- subset(ma_deaths, TOWN20 == 'BOSTON')
head(boston_deaths)
```

Starts with defining the outcome columns:

```{r outcome_columns}
outcome_columns <- list(
  "date" = "date",
  "outcome" = "daily_deaths",
  "factor" = 'age_grp',
  "factor" = 'sex',
  "geo_unit" = "TOWN20",
  "geo_unit_grp" = "COUNTY20"
)
```

Notice that we have several columns of `factors` -- right now we will sum up within these groups, but we will introduce functionality later to run models across multiple factors.   

As with the exposure dataset, there may be some days that are missing data, and we want to aggregate these data across the factors we aren't using and to the correct spatial unit for this analysis. The `make_outcome_table` function accomplishes these goals, 

```{r make_outcome_table}
boston_deaths_tbl <- make_outcome_table(boston_deaths,  outcome_columns)
head(boston_deaths_tbl)
```

Notice that two variables have been added: 

(1) the `strata`. typically this is for for geo-unit (or geo-unit-grp), year, month- and day of week (essential for a time- and- space stratified conditional poisson model of heat-heath impacts). If `geo-unit`, then results are estimated at the `geo-unit` level. if `geo-unit-grp`, then results are estimated at the `geo-unit-grp` level.

and (2) a variable `strata` describing the number of total outcomes within each strata

### Basic DLNM (manual walkthrough)

Now, we'll walk through a basic DLNM of heat-health associations for a single zone. This assumes some basic knowledge of `library(dlnm)`.

Here are the libraries we'll need to run a conditional poisson model.
```{r dlnm_libraries, message=FALSE}
library(dlnm)
library(gnm)
library(ggplot2)
library(data.table)
```

Here are the inputs to define the crossbasis matrix. We use a default of `ns` so that the behavior at the the tails of the distribution is linear.
```{r dlnm_other}
arg_fun = 'ns'
lag_fun = 'ns'
x_knots = quantile(boston_exposure_mat$tmax_C, probs = c(0.5, 0.9))
maxlag = 5
nk = 2
```

And put these into the lists for `argvar` and `arglag`:
```{r dlmn_lists}
argvar <- list(fun = arg_fun, knots = x_knots)
arglag <- list(fun = lag_fun, knots = logknots(maxlag, nk = nk))
```

Limit the columns of the exposure matrix to be just the ones up to `maxlag`:
```{r create_cb}
xcols <- c('tmax_C', paste0('explag',1:maxlag))
x_mat <- boston_exposure_mat[, ..xcols]

cb <- crossbasis(x_mat, lag = maxlag, argvar = argvar, arglag = arglag)
```

Now run the model
```{r run_model}
m_sub <- gnm(daily_deaths ~ cb,
             data = boston_deaths_tbl,
             family = quasipoisson,
             eliminate = factor(strata),
             subset = strata_total > 0)
summary(m_sub)
```

Notice that the dispersion parameter is quite high! but again, these are simulated data -- this is part of how the quasipoisson model works and is used to re-scale the variance covariance matrix.

Next get the crosspred outputs correctly centered at the minimum RR
```{r crosspred}
cp <- crosspred(cb, m_sub, cen = 20, by = 0.1)

cen = cp$predvar[which.min(cp$allRRfit)]

cp <- crosspred(cb, m_sub, cen = cen, by = 0.1)
```

And plot
```{r cp_plot, fig.height=3, fig.width=5}
plot_cp = data.frame(
    x = cp$predvar,
    RR = cp$allRRfit,
    RRlow = cp$allRRlow,
    RRhigh = cp$allRRhigh
)

ggplot(plot_cp, aes(x = x, y = RR, ymin = RRlow, ymax = RRhigh)) +
  geom_hline(yintercept = 1, linetype = '11') +
  theme_classic() +
  geom_ribbon(fill = 'grey75', alpha = 0.2) +
  geom_line() + xlab("Tmax (degC)")
```

Finally, do some basic checks of the math using other utility functions to estimate the dispersion parameters and the variance-covariance matrix. Both of these functions will be useful once we are no longer using R functions to estimate the model.

First to check the dispersion
```{r compute_dispersion}
dispersion <- calc_dispersion(y = boston_deaths_tbl$daily_deaths, 
                X = cb, 
                stratum_vector = boston_deaths_tbl$strata, 
                beta = coef(m_sub))
dispersion
```

Next to check vcov
```{r compute_vcov}
vcov_beta <- calc_vcov(y = boston_deaths_tbl$daily_deaths, 
                X = cb, 
                stratum_vector = boston_deaths_tbl$strata, 
                beta = coef(m_sub))

# update with the dipserion parameter
t1 <- vcov_beta * dispersion

# check against the original model's vcov
t2 <- vcov(m_sub)
attributes(t2) <- NULL
t2 <- matrix(t2, nrow = nrow(t1), ncol = ncol(t1))

# should be the same
all.equal(t1, t2, tolerance = 1e-8)
```

### Basic DLNM (using `cityHeatHealth` functions in `R`)

The functionality above is what is encapsulated in the `condPois_single` function. This is one of the chief innovations of this package. Starting from a messy exposure and outcome dataset, we can quickly estimate heat-health impacts in 3 lines.

We have built-in defaults for `argvar`, `arglag`, and `maxlag` for the investigation of warm-season non-fatal health impacts associated with increases in temperature. We don't need to check the dispersion parameter here since this is using `gnm` under the hood.

```{r single_zone}
# create exposure matrix
exposure_columns <- list(
  "date" = "date",
  "exposure" = "tmax_C",
  "geo_unit" = "TOWN20",
  "geo_unit_grp" = "COUNTY20"
)
boston_exposure_mat <- make_exposure_matrix(boston_exposure, exposure_columns)

# create outcome table
outcome_columns <- list(
  "date" = "date",
  "outcome" = "daily_deaths",
  "factor" = 'age_grp',
  "factor" = 'sex',
  "geo_unit" = "TOWN20",
  "geo_unit_grp" = "COUNTY20"
)
boston_deaths_tbl <- make_outcome_table(boston_deaths,  outcome_columns)

# run the model
m1 <- condPois_single(exposure_matrix = boston_exposure_mat, 
                  outcomes_tbl = boston_deaths_tbl)

```

And plot
```{r single_plot, fig.height=3, fig.width=5}
plot(m1)
```
